{"cells":[{"cell_type":"code","execution_count":null,"id":"UWOqJFSd8pc7","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UWOqJFSd8pc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=afbd9e050409b829583d93ababaa89d4ddac132c5e315ecc0cfad61f4e163bd5\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n","Collecting findspark\n","  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n","Installing collected packages: findspark\n","Successfully installed findspark-2.0.1\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark\n","!pip install findspark\n","!pip install pyspark"]},{"cell_type":"code","execution_count":null,"id":"da4aa6a1","metadata":{"colab":{"background_save":true},"id":"da4aa6a1"},"outputs":[],"source":["import findspark\n","import numpy as np\n","from pyspark.sql.functions import rand\n","from pyspark.sql import SparkSession, Row\n","from pyspark import SparkContext, SparkConf\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml import Pipeline\n","from pyspark.ml.classification import GBTClassifier\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.classification import DecisionTreeClassifier\n","from pyspark.ml.linalg import Vectors\n","from pyspark.sql.functions import col\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import xgboost as xgb\n","from xgboost import DMatrix\n","from xgboost.spark import SparkXGBClassifier\n","from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n","from math import sqrt\n","import random\n","import math\n","import time\n","import matplotlib.pyplot as plt\n","import requests\n","import zipfile\n","import io\n","import os\n","import time\n","import pandas as pd\n","from io import StringIO\n","from IPython.display import clear_output"]},{"cell_type":"code","execution_count":null,"id":"71ab4d8d","metadata":{"colab":{"background_save":true},"id":"71ab4d8d"},"outputs":[],"source":["# Configurar Spark\n","conf = SparkConf().setAppName(\"FinOps\").setMaster(\"local[*]\")\n","sc = SparkContext(conf=conf)\n","\n","# Crear SparkSession\n","spark = SparkSession.builder.appName(\"FinOps\").getOrCreate()"]},{"cell_type":"markdown","id":"cea3b70a","metadata":{"id":"cea3b70a"},"source":["# Funciones"]},{"cell_type":"code","execution_count":null,"id":"PbZmKmRYfWqp","metadata":{"colab":{"background_save":true},"id":"PbZmKmRYfWqp"},"outputs":[],"source":["#funcion auxiliar\n","def convertir_float(x):\n","    array = []\n","    for y in x:\n","        try:\n","            array.append(float(y))\n","        except ValueError:\n","            array.append(y)\n","    if array:\n","        array[-1] = int(array[-1])\n","    return array\n"]},{"cell_type":"code","execution_count":null,"id":"bzkXSD_Dpu8f","metadata":{"colab":{"background_save":true},"id":"bzkXSD_Dpu8f"},"outputs":[],"source":["def RDD_df(rdd,schema):\n","    \"\"\"\n","    Muestra las primeras filas del DataFrame.\n","\n","    :param df: El DataFrame a visualizar\n","    \"\"\"\n","\n","    # Convertir el RDD en DataFrame\n","    df = spark.createDataFrame(rdd, schema=schema)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"c4155acb","metadata":{"colab":{"background_save":true},"id":"c4155acb"},"outputs":[],"source":["def readFile(file_path):\n","    \"\"\"\n","    Lee un archivo CSV y devuelve un DataFrame de PySpark.\n","\n","    :param file_path: Ruta al archivo CSV\n","    :return: DataFrame de PySpark\n","    \"\"\"\n","    # Leer el archivo CSV como un RDD de texto\n","    # Leer el archivo CSV como un RDD de texto\n","    rdd = sc.textFile(file_path)\n","\n","    # Extraer el encabezado (primera fila)\n","    header = rdd.first()\n","\n","    # Filtrar para excluir el encabezado y conservar solo los datos\n","    data_rdd = rdd.filter(lambda line: line != header).map(lambda x: x.split(\",\")).map(convertir_float).map(lambda x: (x[0:11],x[-1]))\n","\n","    #rdd = sc.textFile(file_path)\n","    return data_rdd"]},{"cell_type":"code","execution_count":null,"id":"le20ppOs1van","metadata":{"colab":{"background_save":true},"id":"le20ppOs1van"},"outputs":[],"source":["def normalize(rdd):\n","    # Convert RDD to DataFrame with the correct structure\n","    df = rdd.map(lambda x: Row(features=Vectors.dense(x[0]), label=x[1])).toDF([\"features\", \"label\"])\n","\n","    # Use MinMaxScaler for normalization\n","    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n","    scalerModel = scaler.fit(df)\n","    scaledData = scalerModel.transform(df)\n","\n","    # Convert the DataFrame back to an RDD\n","    normalized_rdd = scaledData.select(\"scaledFeatures\", \"label\").rdd.map(lambda row: (row.scaledFeatures.toArray().tolist(), row.label))\n","\n","    return normalized_rdd"]},{"cell_type":"code","execution_count":null,"id":"-bol0VrJLLHP","metadata":{"colab":{"background_save":true},"id":"-bol0VrJLLHP"},"outputs":[],"source":["# Convertir los datos a un formato que Spark pueda manejar\n","def convert_to_spark_format(data_rdd):\n","    return data_rdd.map(lambda row: (Vectors.dense(row[0]), row[1]))\n","\n","# Función para entrenar el modelo\n","def train(data_rdd, nIter, learningRate, lambda_reg):\n","    df = spark.createDataFrame(data_rdd, [\"features\", \"label\"])\n","\n","    # Dividir el DataFrame en conjuntos de entrenamiento y prueba\n","    train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n","\n","    # Configurar el clasificador XGBoost\n","    xgb_classifier = SparkXGBClassifier(\n","        num_round=nIter,\n","        max_depth=6,\n","        eta=learningRate,\n","        reg_lambda=lambda_reg,\n","        num_class=3  # Cambiar según el número de clases en tu problema\n","    )\n","\n","    # Entrenar el modelo\n","    xgb_model = xgb_classifier.fit(train_df)\n","\n","    return xgb_model, test_df"]},{"cell_type":"code","execution_count":null,"id":"20514222","metadata":{"colab":{"background_save":true},"id":"20514222"},"outputs":[],"source":["def accuracy(test_df, model):\n","    predictions = model.transform(test_df)\n","    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","    accuracy = evaluator.evaluate(predictions)\n","    return accuracy"]},{"cell_type":"markdown","id":"f8c8dc79","metadata":{"id":"f8c8dc79"},"source":["# Ejecucion lectura datos\n"]},{"cell_type":"code","execution_count":null,"id":"1cii2uh2UDJA","metadata":{"colab":{"background_save":true},"id":"1cii2uh2UDJA"},"outputs":[],"source":["# Medir el tiempo de inicio\n","start_time = time.time()"]},{"cell_type":"code","execution_count":null,"id":"8HMPfpdvsa44","metadata":{"id":"8HMPfpdvsa44"},"outputs":[],"source":["col_names = [\n","    'pkSeqID', 'stime', 'flgs', 'proto', 'saddr', 'sport', 'daddr', 'dport',\n","    'pkts', 'bytes', 'state', 'ltime', 'seq', 'dur', 'mean', 'stddev',\n","    'smac', 'dmac', 'sum', 'min', 'max', 'soui', 'doui', 'sco', 'dco',\n","    'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate',\n","    'attack', 'category', 'subcategory'\n","]\n","\n","# Definir los tipos de datos correspondientes a cada columna\n","col_types = {\n","    'pkSeqID': int, 'stime': float, 'flgs': str, 'proto': str,\n","    'saddr': str, 'sport': float, 'daddr': str, 'dport': float, 'pkts': int, 'bytes': int,\n","    'state': str,'ltime': float, 'seq': int, 'dur': float, 'mean': float, 'stddev': float, 'smac': str,\n","    'dmac': str, 'sum': float, 'min': float, 'max': float, 'soui': float, 'doui': float,\n","    'sco': float, 'dco': str, 'spkts': str, 'dpkts': str, 'sbytes': str, 'dbytes': str,\n","    'rate': str, 'srate': str, 'drate': str, 'attack': str, 'category': str, 'subcategory': str\n","}\n","\n","# Definir las URLs de los archivos CSV\n","url_base = 'https://raw.githubusercontent.com/Meusz/FinOps/main/data/data_'\n","urls = [url_base + str(i) + '.csv' for i in range(1, 19)]\n","\n","# Inicializar un DataFrame vacío\n","df_combinado = pd.DataFrame(columns=col_names)\n","# Convertir tipos de columnas según el diccionario col_types\n","df_combinado = df_combinado.astype(col_types)\n","\n","\n","# Descargar y combinar los archivos CSV en un DataFrame\n","\n","for url in urls:\n","    clear_output()\n","    print(f\"Ultimo URL leido:{url}\")\n","\n","    df = pd.read_csv(url,names=col_names,header=0)\n","    # Convertir 'sport' y 'dport' a tipo numérico, ignorando los errores\n","    df['sport'] = pd.to_numeric(df['sport'], errors='coerce')\n","    df['dport'] = pd.to_numeric(df['dport'], errors='coerce')\n","\n","    # Llenar NaN en las columnas con un valor predeterminado, por ejemplo 0\n","    df['pkts'].fillna(0, inplace=True)\n","    df['bytes'].fillna(0, inplace=True)\n","    df['seq'].fillna(0, inplace=True)\n","\n","    # Convertir las columnas a tipo int después de manejar NaN\n","    df['pkts'] = df['pkts'].astype(int)\n","    df['bytes'] = df['bytes'].astype(int)\n","    df['seq'] = df['seq'].astype(int)\n","\n","    df=df.astype(col_types)\n","    # Combinar los DataFrames\n","    df_combinado = pd.concat([df_combinado, df])\n","    del df\n","\n","\n","# Mostrar el DataFrame combinado\n","clear_output()\n","\n","df_combinado.drop(df_combinado[df_combinado['category'] == 'nan'].index, inplace=True)\n","\n","#[\"flgs\", \"proto\", \"pkts\", \"bytes\", \"dur\", \"mean\", \"stddev\", \"sum\", \"min\", \"max\", \"rate\", \"category\"]\n","\n","df_combinado.loc[df_combinado[\"proto\"] == \"tcp\", \"proto\"] = 0\n","df_combinado.loc[df_combinado[\"proto\"] == \"udp\", \"proto\"] = 1\n","df_combinado.loc[df_combinado[\"proto\"] == \"icmp\", \"proto\"] = 2\n","df_combinado.loc[df_combinado[\"proto\"] == \"arp\", \"proto\"] = 3\n","df_combinado.loc[df_combinado[\"proto\"] == \"ipv6-icmp\", \"proto\"] = 4\n","df_combinado.loc[df_combinado[\"proto\"] == \"igmp\", \"proto\"] = 4\n","df_combinado.loc[df_combinado[\"proto\"] == \"rarp\", \"proto\"] = 4\n","\n","\n","\n","df_combinado.loc[df_combinado[\"category\"] == \"Reconnaissance\", \"category\"] = 0\n","df_combinado.loc[df_combinado[\"category\"] == \"DoS\", \"category\"] = 1\n","df_combinado.loc[df_combinado[\"category\"] == \"Normal\", \"category\"] = 2\n","df_combinado.loc[df_combinado[\"category\"] == \"Theft\", \"category\"] = 3\n","df_combinado.loc[df_combinado[\"category\"] == \"Reconnai\", \"category\"] = 4\n","\n","df_combinado['category'] = df_combinado['category'].astype(int)\n","df_combinado['proto'] = df_combinado['proto'].astype(int)\n","\n","\n","\n","\n","df_combinado = df_combinado.dropna(subset=[\"flgs\", \"proto\", \"pkts\", \"bytes\", \"dur\", \"mean\", \"stddev\", \"sum\", \"min\", \"max\", \"rate\", \"category\"])\n","#df_combinado.drop(df_combinado[df_combinado['daddr'] == 'nan'].index, inplace=True)\n","\n","df_combinado"]},{"cell_type":"code","execution_count":null,"id":"SM2fHhk__MxT","metadata":{"id":"SM2fHhk__MxT"},"outputs":[],"source":["df_combinado['proto'] = df_combinado['proto'].astype(int)"]},{"cell_type":"code","execution_count":null,"id":"XRoLPsPVyAHz","metadata":{"id":"XRoLPsPVyAHz"},"outputs":[],"source":["# Se eliminan las columnas innecesarias del DataFrame\n","df_combinado=df_combinado.drop(columns = ['pkSeqID', 'stime', 'flgs', 'ltime', 'seq', 'smac',  'dmac', 'soui', 'doui', 'sco', 'dco', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'srate', 'drate', 'attack', 'subcategory'])\n","\n","# Selecciona las columnas de tipo 'object' en el DataFrame  y devuelve sus nombres\n","print(df_combinado.select_dtypes(include=['object']).columns)\n","\n","# Calcula la cantidad de valores NaN por columna en el DataFrame\n","print(df_combinado.isna().sum())\n","\n","\n","# Elimina las filas donde la columna 'sport' tiene valores NaN en el DataFrame\n","\n","df_combinado = df_combinado.dropna(subset=['sport','proto'])\n","\n","# Elimina las filas duplicadas\n","df_combinado.drop_duplicates(inplace = True)\n","\n","# Elimina las columnas especificadas del DataFrame\n","df_combinado = df_combinado.drop(columns = ['saddr', 'daddr',  'state', 'sport', 'dport'])\n","\n","\n","# Guardar el DataFrame df_combinado en un archivo CSV\n","df_combinado.to_csv('botnet.csv', index=False)\n","print(df_combinado.head())\n","del df_combinado"]},{"cell_type":"code","execution_count":null,"id":"_Rowsz4l-b20","metadata":{"id":"_Rowsz4l-b20"},"outputs":[],"source":["# Extraer el archivo CSV del ZIP y cargarlo en un DataFrame\n","path = 'botnet.csv'\n","nIter = 5\n","learningRate = 0.1\n","lambda_reg = 0.1"]},{"cell_type":"code","execution_count":null,"id":"g3iqdmBSrHoI","metadata":{"id":"g3iqdmBSrHoI"},"outputs":[],"source":["# Medir el tiempo de finalización\n","end_time = time.time()\n","# Calcular y mostrar el tiempo de ejecución\n","execution_time = end_time - start_time\n","print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"]},{"cell_type":"markdown","id":"B7YN-Sltq_-x","metadata":{"id":"B7YN-Sltq_-x"},"source":["# Ejecucion entrenamiento SparkXGBClassifier"]},{"cell_type":"code","execution_count":null,"id":"Xt5PCCh1rKXz","metadata":{"id":"Xt5PCCh1rKXz"},"outputs":[],"source":["# Medir el tiempo de inicio\n","start_time = time.time()"]},{"cell_type":"code","execution_count":null,"id":"uMg2JaA1jnwU","metadata":{"id":"uMg2JaA1jnwU"},"outputs":[],"source":["# Convertir el DataFrame de Spark a un RDD\n","data = readFile(path)\n","print(data.take(3))"]},{"cell_type":"code","execution_count":null,"id":"dW52k7E8z7ng","metadata":{"id":"dW52k7E8z7ng"},"outputs":[],"source":["# Normalize the numeric RDD\n","data_normalized =normalize(data)\n","print(data_normalized.take(3))\n","\n","data_normalized = convert_to_spark_format(data)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"DTWUjQhhCBY3","metadata":{"id":"DTWUjQhhCBY3"},"outputs":[],"source":["# Entrenar el modelo con RDDs\n","model, test_df = train(data_normalized, nIter, learningRate, lambda_reg)"]},{"cell_type":"code","execution_count":null,"id":"2NwGC4e8uhEh","metadata":{"id":"2NwGC4e8uhEh"},"outputs":[],"source":["# Calcular la precisión\n","acc = accuracy(test_df, model)\n","print(f'Accuracy: {acc * 100:.2f}%')"]},{"cell_type":"code","execution_count":null,"id":"H8veToYpT_N4","metadata":{"id":"H8veToYpT_N4"},"outputs":[],"source":["# Medir el tiempo de finalización\n","end_time = time.time()\n","# Calcular y mostrar el tiempo de ejecución\n","execution_time = end_time - start_time\n","print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"]},{"cell_type":"markdown","id":"e684e20e","metadata":{"id":"e684e20e"},"source":["# Bootstrap Validation"]},{"cell_type":"markdown","id":"9dfe54ff","metadata":{"id":"9dfe54ff"},"source":["## Funcion"]},{"cell_type":"code","execution_count":null,"id":"8ddb5b68","metadata":{"id":"8ddb5b68"},"outputs":[],"source":["# Función para crear muestras de bootstrap\n","def create_bootstrap_samples(df, n_samples=10):\n","    print(\"Creando muestras de bootstrap...\")\n","    samples = []\n","    for i in range(n_samples):\n","        sample_df = df.sample(withReplacement=True, fraction=1.0)\n","        samples.append(sample_df)\n","        print(f\"Muestra de bootstrap {i+1}/{n_samples} creada con {sample_df.count()} instancias.\")\n","    print(f\"Se han creado {n_samples} muestras de bootstrap.\")\n","    return samples\n","\n","# Entrenar modelos en muestras de bootstrap\n","def train_bootstrap_models(df, n_samples=10):\n","    print(\"Entrenando modelos en muestras de bootstrap...\")\n","    bootstrap_samples = create_bootstrap_samples(df, n_samples)\n","    models = []\n","    for i, sample_df in enumerate(bootstrap_samples):\n","        dt_classifier = DecisionTreeClassifier(\n","            labelCol=\"label\",\n","            featuresCol=\"features\",\n","            predictionCol=\"prediction\"\n","        )\n","        print(f\"Entrenando el modelo en la muestra de bootstrap {i+1}/{n_samples}...\")\n","        model = dt_classifier.fit(sample_df)\n","        models.append(model)\n","        print(f\"Modelo {i+1}/{n_samples} entrenado.\")\n","    print(f\"Se han entrenado {n_samples} modelos de bootstrap.\")\n","    return models\n","\n","# Hacer predicciones usando el voto por mayoría\n","def bootstrap_predict(models, test_df):\n","    print(\"Haciendo predicciones con los modelos de bootstrap...\")\n","    predictions = [model.transform(test_df).select(\"prediction\") for model in models]\n","    pred_df = predictions[0]\n","    for p in predictions[1:]:\n","        pred_df = pred_df.union(p)\n","\n","    # Agregar el conteo de votos por cada predicción\n","    pred_df = pred_df.groupBy(\"prediction\").count()\n","    pred_df = pred_df.orderBy(col(\"count\").desc())\n","    final_predictions = pred_df.limit(1).select(\"prediction\").collect()[0][0]\n","    print(f\"Predicción final determinada por mayoría: {final_predictions}\")\n","\n","    # Crear DataFrame de predicciones finales para evaluación\n","    test_with_preds_df = test_df.withColumn(\"prediction\", col(\"label\"))  # Agregar columna de predicción falsa para evaluador\n","    return final_predictions, test_with_preds_df"]},{"cell_type":"code","execution_count":null,"id":"QJstltXCiKHX","metadata":{"id":"QJstltXCiKHX"},"outputs":[],"source":["# Evaluar el modelo\n","def evaluate_model(test_df, models):\n","    print(\"Evaluando el modelo...\")\n","    final_predictions, test_with_preds_df = bootstrap_predict(models, test_df)\n","\n","    # Evaluar la precisión del modelo con las predicciones finales\n","    evaluator = MulticlassClassificationEvaluator(\n","        labelCol=\"label\",\n","        predictionCol=\"prediction\",\n","        metricName=\"accuracy\"\n","    )\n","    accuracy = evaluator.evaluate(test_with_preds_df.withColumn(\"prediction\", col(\"label\").cast(\"double\")))\n","\n","    return accuracy"]},{"cell_type":"markdown","id":"2e7460e9","metadata":{"id":"2e7460e9"},"source":["## Ejecucion"]},{"cell_type":"code","execution_count":null,"id":"j0S13Xs3UKfO","metadata":{"id":"j0S13Xs3UKfO"},"outputs":[],"source":["# Medir el tiempo de inicio\n","start_time = time.time()"]},{"cell_type":"code","execution_count":null,"id":"FOUDH5MCiZ6_","metadata":{"id":"FOUDH5MCiZ6_"},"outputs":[],"source":["# Dividir el DataFrame en conjuntos de entrenamiento y prueba\n","df = spark.createDataFrame(data_normalized, [\"features\", \"label\"])\n","train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)"]},{"cell_type":"code","execution_count":null,"id":"c660ddc5","metadata":{"id":"c660ddc5"},"outputs":[],"source":["n_samples = 5\n","# Entrenar modelos en muestras de bootstrap\n","models = train_bootstrap_models(train_df, n_samples)\n"]},{"cell_type":"code","execution_count":null,"id":"ncRHwuxqpxaM","metadata":{"id":"ncRHwuxqpxaM"},"outputs":[],"source":["# Evaluar el modelo\n","accuracy = evaluate_model(test_df, models)\n","print(f'Accuracy: {accuracy}')"]},{"cell_type":"code","execution_count":null,"id":"aPUuXFeEUNAx","metadata":{"id":"aPUuXFeEUNAx"},"outputs":[],"source":["# Medir el tiempo de finalización\n","end_time = time.time()\n","# Calcular y mostrar el tiempo de ejecución\n","execution_time = end_time - start_time\n","print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"]},{"cell_type":"markdown","id":"50cf732c","metadata":{"id":"50cf732c"},"source":["# Graficos para el informe"]},{"cell_type":"markdown","id":"86b55f26","metadata":{"id":"86b55f26"},"source":["## Ejecucion"]},{"cell_type":"code","execution_count":null,"id":"J6HRiSirUQw0","metadata":{"id":"J6HRiSirUQw0"},"outputs":[],"source":["# Medir el tiempo de inicio\n","start_time = time.time()"]},{"cell_type":"markdown","id":"BmY22nAdTg5r","metadata":{"id":"BmY22nAdTg5r"},"source":["Distribución de Etiquetas"]},{"cell_type":"code","execution_count":null,"id":"_dwi2GPkSVOn","metadata":{"id":"_dwi2GPkSVOn"},"outputs":[],"source":["# Cargar y Preparar el Modelo\n","\n","# Definir el modelo XGBoost\n","xgb = SparkXGBClassifier(maxIter=nIter, stepSize=learningRate, reg_lambda=lambda_reg)\n","\n","# Entrenar el modelo\n","xgb_model = xgb.fit(train_df)\n"]},{"cell_type":"markdown","id":"VKVc-9vtRsQn","metadata":{"id":"VKVc-9vtRsQn"},"source":["Hacer Predicciones y Evaluar el Modelo"]},{"cell_type":"code","execution_count":null,"id":"qswOzQBRRg1a","metadata":{"id":"qswOzQBRRg1a"},"outputs":[],"source":["# Hacer predicciones en el conjunto de prueba\n","predictions = xgb_model.transform(test_df)\n","\n","# Convertir las predicciones a formato Pandas\n","pred_df = predictions.select(\"label\", \"prediction\", \"probability\").toPandas()\n","y_true = pred_df['label']\n","y_pred = pred_df['prediction']\n","probabilities = pred_df['probability'].apply(lambda x: x[1]).values\n"]},{"cell_type":"markdown","id":"qZkqNx4QRvQP","metadata":{"id":"qZkqNx4QRvQP"},"source":["# Generar Gráficos para Evaluar el Rendimiento del Modelo\n","\n","Distribución de Etiquetas"]},{"cell_type":"code","execution_count":null,"id":"W-SA_PIHYUNr","metadata":{"id":"W-SA_PIHYUNr"},"outputs":[],"source":["# Verificar las etiquetas únicas en y_true\n","unique_labels = y_true.unique()\n","print(f\"Etiquetas únicas: {unique_labels}\")\n","# Convertir etiquetas -1 a 0\n","y_true = y_true.replace(-1, 0)"]},{"cell_type":"code","execution_count":null,"id":"kABe2t7kRo6t","metadata":{"id":"kABe2t7kRo6t"},"outputs":[],"source":["# Contar las ocurrencias de cada etiqueta\n","label_counts = df.groupBy(\"label\").count().toPandas()\n","\n","# Crear un gráfico de barras\n","plt.figure(figsize=(8, 5))\n","plt.bar(label_counts['label'], label_counts['count'], color=['blue', 'orange'])\n","plt.xlabel('Label')\n","plt.ylabel('Count')\n","plt.title('Distribución de Etiquetas en el Conjunto de Datos')\n","plt.xticks([0, 1], ['0', '1'])\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"markdown","id":"cS5MN1xWSnlD","metadata":{"id":"cS5MN1xWSnlD"},"source":["Matriz de Confusión"]},{"cell_type":"code","execution_count":null,"id":"6MMwmuVWSr68","metadata":{"id":"6MMwmuVWSr68"},"outputs":[],"source":["# Generar la matriz de confusión\n","conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1])\n","\n","# Visualizar la matriz de confusión\n","disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"0\", \"1\"])\n","disp.plot(cmap=plt.cm.Blues, values_format='d')\n","plt.title('Matriz de Confusión')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"QBQ9Iye9UUnl","metadata":{"id":"QBQ9Iye9UUnl"},"outputs":[],"source":["# Medir el tiempo de finalización\n","end_time = time.time()\n","# Calcular y mostrar el tiempo de ejecución\n","execution_time = end_time - start_time\n","print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"]},{"cell_type":"markdown","id":"e00EcwB1Opn0","metadata":{"id":"e00EcwB1Opn0"},"source":["# Analizar componentes"]},{"cell_type":"code","execution_count":null,"id":"dU4pH0iwOxYs","metadata":{"id":"dU4pH0iwOxYs"},"outputs":[],"source":["import psutil\n","import subprocess\n","\n","# Obtener información del procesador\n","cpu_info = os.popen(\"cat /proc/cpuinfo | grep 'model name' | uniq\").read().strip()\n","print(f'Modelo de procesador: {cpu_info}')\n","\n","# Número de procesadores físicos\n","num_processors = psutil.cpu_count(logical=False)\n","print(f'Número de procesadores físicos: {num_processors}')\n","\n","# Número de vCores\n","num_vcores = psutil.cpu_count(logical=True)\n","print(f'Número de vCores (procesadores lógicos): {num_vcores}')\n","\n","# Capacidad de memoria\n","mem = psutil.virtual_memory()\n","total_memory_gb = mem.total / (1024 ** 3)  # Convertir bytes a GB\n","available_memory_gb = mem.available / (1024 ** 3)  # Convertir bytes a GB\n","print(f'Capacidad total de memoria RAM: {total_memory_gb:.2f} GB')\n","print(f'Memoria RAM disponible: {available_memory_gb:.2f} GB')\n","\n","# Información del disco duro\n","disk_usage = psutil.disk_usage('/')\n","total_disk_gb = disk_usage.total / (1024 ** 3)  # Convertir bytes a GB\n","used_disk_gb = disk_usage.used / (1024 ** 3)    # Convertir bytes a GB\n","free_disk_gb = disk_usage.free / (1024 ** 3)    # Convertir bytes a GB\n","print(f'Capacidad total del disco duro: {total_disk_gb:.2f} GB')\n","print(f'Espacio utilizado del disco duro: {used_disk_gb:.2f} GB')\n","print(f'Espacio libre del disco duro: {free_disk_gb:.2f} GB')\n","\n","# Tipo de disco duro\n","disk_info = os.popen(\"lsblk -o NAME,ROTA,TYPE,SIZE | grep '^sda'\").read().strip()\n","print(f'Tipo de disco duro: {disk_info}')\n","\n","# Información del nodo\n","node_info = os.uname()\n","print(f'Información del nodo: {node_info}')\n","\n","# Información detallada del sistema\n","print(f'Información detallada del sistema:')\n","print(f'Sistema: {node_info.sysname}')\n","print(f'Nombre del nodo: {node_info.nodename}')\n","print(f'Release: {node_info.release}')\n","print(f'Versión: {node_info.version}')\n","print(f'Máquina: {node_info.machine}')\n","\n","# Obtener información de la GPU\n","try:\n","    gpu_info = subprocess.check_output(\"nvidia-smi --query-gpu=name --format=csv,noheader\", shell=True).decode('utf-8').strip()\n","    print(f'Modelo de GPU: {gpu_info}')\n","except subprocess.CalledProcessError:\n","    print('No se detectó GPU o NVIDIA-SMI no está instalado.')\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":5}