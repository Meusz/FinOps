{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "UWOqJFSd8pc7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWOqJFSd8pc7",
        "outputId": "080ef5d8-f8ae-4e17-c266-073234b31953"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=72e06ca0a0b7280fea53f5a6452d503e24e006fdcea44d61a20004acdd1db1e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da4aa6a1",
      "metadata": {
        "id": "da4aa6a1"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "import numpy as np\n",
        "from pyspark.sql.types import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler\n",
        "from math import sqrt\n",
        "findspark.init()\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "from io import StringIO\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "71ab4d8d",
      "metadata": {
        "id": "71ab4d8d"
      },
      "outputs": [],
      "source": [
        "# Configurar Spark\n",
        "conf = SparkConf().setAppName(\"FinOps\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# Crear SparkSession\n",
        "spark = SparkSession.builder.appName(\"FinOps\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6bdfacb",
      "metadata": {
        "id": "f6bdfacb"
      },
      "source": [
        "# Funciones Aux"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9bprYH8jiDzA",
      "metadata": {
        "id": "9bprYH8jiDzA"
      },
      "outputs": [],
      "source": [
        "#funcion auxiliar\n",
        "def convertir_float(x):\n",
        "    array = []\n",
        "    for y in x:\n",
        "        try:\n",
        "            array.append(float(y))\n",
        "        except ValueError:\n",
        "            array.append(y)\n",
        "    if array:\n",
        "        array[-1] = int(array[-1])\n",
        "    return array\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "238e05ce",
      "metadata": {
        "id": "238e05ce"
      },
      "outputs": [],
      "source": [
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def get_y_hat(features, w):\n",
        "    # Assuming features is an array of features, w is a matrix of shape (1, 11)\n",
        "    return np.dot(features, w.T)  # Ensure w.T if w is (1, 11) and features is compatible\n",
        "\n",
        "def grad_cost(label, features, y_hat):\n",
        "    # Calculate gradient of cost function w.r.t. weights\n",
        "    return (y_hat - label) * features\n",
        "\n",
        "\n",
        "def get_derivatives(row, weights):\n",
        "    features = row[:-1]\n",
        "    y = row[-1]\n",
        "    y_hat = get_y_hat(features, weights)\n",
        "    dJ_dw = (y_hat - y) * np.append(features, 1)\n",
        "    return dJ_dw\n",
        "\n",
        "def update_ws(weights, dw, learning_rate):\n",
        "    return weights - learning_rate * dw\n",
        "\n",
        "\n",
        "def fcost(y, y_hat):\n",
        "    #print (\"cost:\",y,y_hat)\n",
        "    # compute loss/cost for one element \"y_hat\" and one label \"y\"\n",
        "    epsilon=0.00000001\n",
        "    if y == 1:\n",
        "        return -np.log(y_hat if y_hat > 0. else epsilon)\n",
        "    else:\n",
        "        return -np.log (1-y_hat if 1-y_hat >0. else epsilon)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cea3b70a",
      "metadata": {
        "id": "cea3b70a"
      },
      "source": [
        "# Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bzkXSD_Dpu8f",
      "metadata": {
        "id": "bzkXSD_Dpu8f"
      },
      "outputs": [],
      "source": [
        "def RDD_df(rdd,schema):\n",
        "    \"\"\"\n",
        "    Muestra las primeras filas del DataFrame.\n",
        "\n",
        "    :param df: El DataFrame a visualizar\n",
        "    \"\"\"\n",
        "\n",
        "    # Convertir el RDD en DataFrame\n",
        "    df = spark.createDataFrame(rdd, schema=schema)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c4155acb",
      "metadata": {
        "id": "c4155acb"
      },
      "outputs": [],
      "source": [
        "def readFile(file_path):\n",
        "    \"\"\"\n",
        "    Lee un archivo CSV y devuelve un DataFrame de PySpark.\n",
        "\n",
        "    :param file_path: Ruta al archivo CSV\n",
        "    :return: DataFrame de PySpark\n",
        "    \"\"\"\n",
        "    # Leer el archivo CSV como un RDD de texto\n",
        "    # Leer el archivo CSV como un RDD de texto\n",
        "    rdd = sc.textFile(file_path)\n",
        "\n",
        "    # Extraer el encabezado (primera fila)\n",
        "    header = rdd.first()\n",
        "\n",
        "    # Filtrar para excluir el encabezado y conservar solo los datos\n",
        "    data_rdd = rdd.filter(lambda line: line != header).map(lambda x: x.split(\",\")).map(convertir_float).map(lambda x: (x[0:11],x[-1]))\n",
        "\n",
        "    #rdd = sc.textFile(file_path)\n",
        "    return data_rdd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "le20ppOs1van",
      "metadata": {
        "id": "le20ppOs1van"
      },
      "outputs": [],
      "source": [
        "def normalize(rdd):\n",
        "    # Convert RDD to DataFrame with the correct structure\n",
        "    df = rdd.map(lambda x: Row(features=Vectors.dense(x[0]), label=x[1])).toDF([\"features\", \"label\"])\n",
        "\n",
        "    # Use MinMaxScaler for normalization\n",
        "    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "    scalerModel = scaler.fit(df)\n",
        "    scaledData = scalerModel.transform(df)\n",
        "\n",
        "    # Convert the DataFrame back to an RDD\n",
        "    normalized_rdd = scaledData.select(\"scaledFeatures\", \"label\").rdd.map(lambda row: (row.scaledFeatures.toArray().tolist(), row.label))\n",
        "\n",
        "    return normalized_rdd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
        "    # Initialize weight vector\n",
        "    w = np.random.randn(1, 11)\n",
        "\n",
        "    for i in range(iterations):\n",
        "                # Compute y_hat using map operation\n",
        "        rdd_y_hat = RDD_Xy.map(lambda x: (x[0], x[1], get_y_hat(x[0], w)))  # x[0] is features, x[1] is label\n",
        "\n",
        "        # Calculate cost and regularization\n",
        "        reg_term = lambda_reg * np.sum(w[:-1] ** 2)  # Regularization term\n",
        "\n",
        "        rdd_fcost = rdd_y_hat.map(lambda x: fcost(x[1], x[2]))  # Cost function value for each sample\n",
        "        J = rdd_fcost.reduce(lambda x, y: x + y)  # Sum up cost function across RDD\n",
        "        J += reg_term  # Add regularization term to total cost\n",
        "        J = J[0]\n",
        "\n",
        "        # Update weights\n",
        "        grad = rdd_y_hat.map(lambda x: grad_cost(x[1], x[0], x[2]))  # x[1] is label, x[0] is features, x[2] is y_hat\n",
        "        grad_sum = grad.reduce(lambda x, y: x + y)  # Sum up gradients across RDD\n",
        "\n",
        "        w = w - learning_rate * grad_sum - reg_term  # Update weight vector\n",
        "        print(f\"Iteration {i}  Cost:  {J}\")\n",
        "\n",
        "    return w"
      ],
      "metadata": {
        "id": "-bol0VrJLLHP"
      },
      "id": "-bol0VrJLLHP",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "20514222",
      "metadata": {
        "id": "20514222"
      },
      "outputs": [],
      "source": [
        "def predict(w,x):\n",
        "    threshold = 0.5\n",
        "    y_hat = get_y_hat(x,w)\n",
        "    return 1 if y_hat > threshold else 0\n",
        "\n",
        "def accuracy(RDD_Xy,w):\n",
        "    correct_answer= 0\n",
        "\n",
        "    def count_correct(Xy):\n",
        "        x,y = Xy\n",
        "        return 1 if predict(w,x) == y else 0\n",
        "    correct_answers = RDD_Xy.map(count_correct).reduce(lambda x,y: x+y)\n",
        "    return correct_answers*100/RDD_Xy.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8c8dc79",
      "metadata": {
        "id": "f8c8dc79"
      },
      "source": [
        "# Ejecucion lectura datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1cii2uh2UDJA",
      "metadata": {
        "id": "1cii2uh2UDJA"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de inicio\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8HMPfpdvsa44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HMPfpdvsa44",
        "outputId": "cae78770-1542-4fcd-e359-4a9febffd2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultimo URL leido:https://raw.githubusercontent.com/Meusz/FinOps/main/data/data_14.csv\n"
          ]
        }
      ],
      "source": [
        "col_names = [\n",
        "    'pkSeqID', 'stime', 'flgs', 'proto', 'saddr', 'sport', 'daddr', 'dport',\n",
        "    'pkts', 'bytes', 'state', 'ltime', 'seq', 'dur', 'mean', 'stddev',\n",
        "    'smac', 'dmac', 'sum', 'min', 'max', 'soui', 'doui', 'sco', 'dco',\n",
        "    'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate', 'srate', 'drate',\n",
        "    'attack', 'category', 'subcategory'\n",
        "]\n",
        "\n",
        "# Definir los tipos de datos correspondientes a cada columna\n",
        "col_types = {\n",
        "    'pkSeqID': int, 'stime': float, 'flgs': str, 'proto': str,\n",
        "    'saddr': str, 'sport': float, 'daddr': str, 'dport': float, 'pkts': int, 'bytes': int,\n",
        "    'state': str,'ltime': float, 'seq': int, 'dur': float, 'mean': float, 'stddev': float, 'smac': str,\n",
        "    'dmac': str, 'sum': float, 'min': float, 'max': float, 'soui': float, 'doui': float,\n",
        "    'sco': float, 'dco': str, 'spkts': str, 'dpkts': str, 'sbytes': str, 'dbytes': str,\n",
        "    'rate': str, 'srate': str, 'drate': str, 'attack': str, 'category': str, 'subcategory': str\n",
        "}\n",
        "\n",
        "# Definir las URLs de los archivos CSV\n",
        "url_base = 'https://raw.githubusercontent.com/Meusz/FinOps/main/data/data_'\n",
        "urls = [url_base + str(i) + '.csv' for i in range(1, 19)]\n",
        "\n",
        "# Inicializar un DataFrame vacío\n",
        "df_combinado = pd.DataFrame(columns=col_names)\n",
        "# Convertir tipos de columnas según el diccionario col_types\n",
        "df_combinado = df_combinado.astype(col_types)\n",
        "\n",
        "\n",
        "# Descargar y combinar los archivos CSV en un DataFrame\n",
        "\n",
        "for url in urls:\n",
        "    clear_output()\n",
        "    print(f\"Ultimo URL leido:{url}\")\n",
        "\n",
        "    df = pd.read_csv(url,names=col_names,header=0)\n",
        "    # Convertir 'sport' y 'dport' a tipo numérico, ignorando los errores\n",
        "    df['sport'] = pd.to_numeric(df['sport'], errors='coerce')\n",
        "    df['dport'] = pd.to_numeric(df['dport'], errors='coerce')\n",
        "\n",
        "    # Llenar NaN en las columnas con un valor predeterminado, por ejemplo 0\n",
        "    df['pkts'].fillna(0, inplace=True)\n",
        "    df['bytes'].fillna(0, inplace=True)\n",
        "    df['seq'].fillna(0, inplace=True)\n",
        "\n",
        "    # Convertir las columnas a tipo int después de manejar NaN\n",
        "    df['pkts'] = df['pkts'].astype(int)\n",
        "    df['bytes'] = df['bytes'].astype(int)\n",
        "    df['seq'] = df['seq'].astype(int)\n",
        "\n",
        "    df=df.astype(col_types)\n",
        "    # Combinar los DataFrames\n",
        "    df_combinado = pd.concat([df_combinado, df])\n",
        "    del df\n",
        "\n",
        "\n",
        "# Mostrar el DataFrame combinado\n",
        "clear_output()\n",
        "\n",
        "df_combinado.drop(df_combinado[df_combinado['category'] == 'nan'].index, inplace=True)\n",
        "\n",
        "#[\"flgs\", \"proto\", \"pkts\", \"bytes\", \"dur\", \"mean\", \"stddev\", \"sum\", \"min\", \"max\", \"rate\", \"category\"]\n",
        "\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"tcp\", \"proto\"] = 0\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"udp\", \"proto\"] = 1\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"icmp\", \"proto\"] = 2\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"arp\", \"proto\"] = 3\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"ipv6-icmp\", \"proto\"] = 4\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"igmp\", \"proto\"] = 4\n",
        "df_combinado.loc[df_combinado[\"proto\"] == \"rarp\", \"proto\"] = 4\n",
        "\n",
        "\n",
        "\n",
        "df_combinado.loc[df_combinado[\"category\"] == \"Reconnaissance\", \"category\"] = 0\n",
        "df_combinado.loc[df_combinado[\"category\"] == \"DoS\", \"category\"] = 1\n",
        "df_combinado.loc[df_combinado[\"category\"] == \"Normal\", \"category\"] = 2\n",
        "df_combinado.loc[df_combinado[\"category\"] == \"Theft\", \"category\"] = 3\n",
        "df_combinado.loc[df_combinado[\"category\"] == \"Reconnai\", \"category\"] = 4\n",
        "\n",
        "df_combinado['category'] = df_combinado['category'].astype(int)\n",
        "df_combinado['proto'] = df_combinado['proto'].astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_combinado = df_combinado.dropna(subset=[\"flgs\", \"proto\", \"pkts\", \"bytes\", \"dur\", \"mean\", \"stddev\", \"sum\", \"min\", \"max\", \"rate\", \"category\"])\n",
        "#df_combinado.drop(df_combinado[df_combinado['daddr'] == 'nan'].index, inplace=True)\n",
        "\n",
        "df_combinado"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_combinado['proto'] = df_combinado['proto'].astype(int)"
      ],
      "metadata": {
        "id": "SM2fHhk__MxT"
      },
      "id": "SM2fHhk__MxT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XRoLPsPVyAHz",
      "metadata": {
        "id": "XRoLPsPVyAHz"
      },
      "outputs": [],
      "source": [
        "# Se eliminan las columnas innecesarias del DataFrame\n",
        "df_combinado=df_combinado.drop(columns = ['pkSeqID', 'stime', 'flgs', 'ltime', 'seq', 'smac',  'dmac', 'soui', 'doui', 'sco', 'dco', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'srate', 'drate', 'attack', 'subcategory'])\n",
        "\n",
        "# Selecciona las columnas de tipo 'object' en el DataFrame  y devuelve sus nombres\n",
        "print(df_combinado.select_dtypes(include=['object']).columns)\n",
        "\n",
        "# Calcula la cantidad de valores NaN por columna en el DataFrame\n",
        "print(df_combinado.isna().sum())\n",
        "\n",
        "\n",
        "# Elimina las filas donde la columna 'sport' tiene valores NaN en el DataFrame\n",
        "\n",
        "df_combinado = df_combinado.dropna(subset=['sport','proto'])\n",
        "\n",
        "# Elimina las filas duplicadas\n",
        "df_combinado.drop_duplicates(inplace = True)\n",
        "\n",
        "# Elimina las columnas especificadas del DataFrame\n",
        "df_combinado = df_combinado.drop(columns = ['saddr', 'daddr',  'state', 'sport', 'dport'])\n",
        "\n",
        "\n",
        "# Guardar el DataFrame df_combinado en un archivo CSV\n",
        "df_combinado.to_csv('botnet.csv', index=False)\n",
        "print(df_combinado.head())\n",
        "del df_combinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Rowsz4l-b20",
      "metadata": {
        "id": "_Rowsz4l-b20"
      },
      "outputs": [],
      "source": [
        "# Extraer el archivo CSV del ZIP y cargarlo en un DataFrame\n",
        "path = 'botnet.csv'\n",
        "nIter = 5\n",
        "learningRate = 0.1\n",
        "lambda_reg = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "g3iqdmBSrHoI",
      "metadata": {
        "id": "g3iqdmBSrHoI"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de finalización\n",
        "end_time = time.time()\n",
        "# Calcular y mostrar el tiempo de ejecución\n",
        "execution_time = end_time - start_time\n",
        "print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B7YN-Sltq_-x",
      "metadata": {
        "id": "B7YN-Sltq_-x"
      },
      "source": [
        "# Ejecucion entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xt5PCCh1rKXz",
      "metadata": {
        "id": "Xt5PCCh1rKXz"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de inicio\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uMg2JaA1jnwU",
      "metadata": {
        "id": "uMg2JaA1jnwU"
      },
      "outputs": [],
      "source": [
        "# Convertir el DataFrame de Spark a un RDD\n",
        "data = readFile(path)\n",
        "print(data.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dW52k7E8z7ng",
      "metadata": {
        "id": "dW52k7E8z7ng"
      },
      "outputs": [],
      "source": [
        "# Normalize the numeric RDD\n",
        "data_normalized =normalize(data)\n",
        "print(data_normalized.take(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DTWUjQhhCBY3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTWUjQhhCBY3",
        "outputId": "0f7b1788-b1f5-401b-eb77-3a24065e7601"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0  Cost:  339395.72780475754\n",
            "Iteration 1  Cost:  18242505.18245464\n",
            "Iteration 2  Cost:  -10858065.28211775\n",
            "Iteration 3  Cost:  29135653.70775554\n",
            "Iteration 4  Cost:  -37223945.96115874\n"
          ]
        }
      ],
      "source": [
        "# Entrenar el modelo con RDDs\n",
        "ws = train(data_normalized, nIter, learningRate, lambda_reg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2NwGC4e8uhEh",
      "metadata": {
        "id": "2NwGC4e8uhEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3698f824-f69a-45aa-c606-5c49ea93dc8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 9.472300880006914\n"
          ]
        }
      ],
      "source": [
        "# Calcular la precisión\n",
        "acc = accuracy(data_normalized, ws)\n",
        "\n",
        "print(\"Accuracy:\", acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H8veToYpT_N4",
      "metadata": {
        "id": "H8veToYpT_N4"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de finalización\n",
        "end_time = time.time()\n",
        "# Calcular y mostrar el tiempo de ejecución\n",
        "execution_time = end_time - start_time\n",
        "print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e684e20e",
      "metadata": {
        "id": "e684e20e"
      },
      "source": [
        "# Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dfe54ff",
      "metadata": {
        "id": "9dfe54ff"
      },
      "source": [
        "## Funcion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e46a59",
      "metadata": {
        "id": "99e46a59"
      },
      "outputs": [],
      "source": [
        "def transforma(RDD_Xy):\n",
        "    import random\n",
        "    #RDD_Xy_con_indice = RDD_Xy.zipWithIndex()\n",
        "    #RDD_Xy_con_clave = RDD_Xy_con_indice.map(lambda x: (x[0],(x[1]%num_block_cv)))\n",
        "    \"\"\"Si zipWithIndex no esta permitido, quiza podemos utilizar randint(0, num_bloques-1) para\n",
        "    generar claves, aun que la proporcion de tamaño de tran y test en este caso no es determinado\"\"\"\n",
        "    RDD_Xy_con_indice = RDD_Xy.map(lambda x: (x,random.randint(0, num_block_cv-1)))\n",
        "    RDD_Xy_con_clave = RDD_Xy_con_indice.map(lambda x: (x[0],(x[1]%num_block_cv)))\n",
        "\n",
        "    return RDD_Xy_con_clave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ddb5b68",
      "metadata": {
        "id": "8ddb5b68"
      },
      "outputs": [],
      "source": [
        "def get_block_data(RDD_Xy, clave):\n",
        "    train = RDD_Xy.flatMap(lambda x: [x] if x[1] != clave else []).map(lambda x: x[0])\n",
        "    test = RDD_Xy.flatMap(lambda x: [x] if x[1] == clave else []).map(lambda x: x[0])\n",
        "\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e7460e9",
      "metadata": {
        "id": "2e7460e9"
      },
      "source": [
        "## Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "j0S13Xs3UKfO",
      "metadata": {
        "id": "j0S13Xs3UKfO"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de inicio\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7312ad97",
      "metadata": {
        "id": "7312ad97"
      },
      "outputs": [],
      "source": [
        "#definir cuantas bloques quiere dividir para cross-validation\n",
        "num_block_cv = 5\n",
        "avg_acc = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c660ddc5",
      "metadata": {
        "id": "c660ddc5"
      },
      "outputs": [],
      "source": [
        "#Ya tenemos data_normalized\n",
        "data_cv = transforma(data_normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "522caa75",
      "metadata": {
        "id": "522caa75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "292b88b3-bef5-4565-9450-7353aef9114a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation con clave:0\n",
            "Iteration 0  Cost:  152712.30279158414\n",
            "Iteration 1  Cost:  13309409.136283476\n",
            "Iteration 2  Cost:  -7875536.918648434\n",
            "Iteration 3  Cost:  23433739.896141876\n",
            "Iteration 4  Cost:  -28217543.288577415\n",
            "acc: 9.472300880006914\n",
            "Cross-Validation con clave:1\n",
            "Iteration 0  Cost:  2487044.061113406\n",
            "Iteration 1  Cost:  15187045.153861243\n",
            "Iteration 2  Cost:  -9215297.435354795\n",
            "Iteration 3  Cost:  23319653.780725002\n",
            "Iteration 4  Cost:  -29529428.13384171\n",
            "acc: 9.472300880006914\n",
            "Cross-Validation con clave:2\n",
            "Iteration 0  Cost:  2507412.7840857347\n",
            "Iteration 1  Cost:  13339787.433115056\n",
            "Iteration 2  Cost:  -6021336.06666621\n",
            "Iteration 3  Cost:  23335591.976482373\n",
            "Iteration 4  Cost:  -29217334.086343374\n",
            "acc: 9.472300880006914\n",
            "Cross-Validation con clave:3\n",
            "Iteration 0  Cost:  2473424.5946685043\n",
            "Iteration 1  Cost:  13942505.406318696\n",
            "Iteration 2  Cost:  -8984413.951583195\n",
            "Iteration 3  Cost:  23291020.56964372\n",
            "Iteration 4  Cost:  -29585499.603707425\n",
            "acc: 9.472300880006914\n",
            "Cross-Validation con clave:4\n",
            "Iteration 0  Cost:  383920.9469631264\n",
            "Iteration 1  Cost:  18969135.13997393\n",
            "Iteration 2  Cost:  -8562196.562038783\n",
            "Iteration 3  Cost:  23368285.25692313\n",
            "Iteration 4  Cost:  -28654194.14856206\n",
            "acc: 9.472300880006914\n",
            "averge acc: 0\n"
          ]
        }
      ],
      "source": [
        "for i in range(num_block_cv):\n",
        "    print(f\"Cross-Validation con clave:{i}\")\n",
        "    train_data,test = get_block_data(data_cv,i)\n",
        "    ws = train(train_data,nIter,learningRate,lambda_reg)\n",
        "    acc = accuracy(data_normalized,ws)\n",
        "    avg_acc += avg_acc\n",
        "    print(\"acc:\" , acc)\n",
        "print(\"averge acc:\" , avg_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aPUuXFeEUNAx",
      "metadata": {
        "id": "aPUuXFeEUNAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c65032-3af5-40e2-eb67-9665f76cccc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tiempo de ejecución: 6614.83 segundos, 110.25  minutos\n"
          ]
        }
      ],
      "source": [
        "# Medir el tiempo de finalización\n",
        "end_time = time.time()\n",
        "# Calcular y mostrar el tiempo de ejecución\n",
        "execution_time = end_time - start_time\n",
        "print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50cf732c",
      "metadata": {
        "id": "50cf732c"
      },
      "source": [
        "# Graficos para el informe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b727018",
      "metadata": {
        "id": "2b727018"
      },
      "source": [
        "## funcion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0053049d",
      "metadata": {
        "id": "0053049d"
      },
      "outputs": [],
      "source": [
        "def train_visualizacion(RDD_Xy, iterations, learning_rate, lambda_reg):\n",
        "    # Initialize weight vector\n",
        "    m = RDD_Xy.count()  # Number of samples in RDD\n",
        "    num_columns = len(RDD_Xy.take(1)[0][0])  # Number of columns in features + 1 (for bias)\n",
        "    w = np.random.randn(1, 11)\n",
        "    costos_entrenamiento = []  # List to store training costs (objective function + regularization)\n",
        "    tiempos = []              # List to store training times\n",
        "    for i in range(iterations):\n",
        "        start_time = time.time()  # Start time of iteration\n",
        "        # Compute y_hat using map operation\n",
        "        rdd_y_hat = RDD_Xy.map(lambda x: (x[0], x[1], get_y_hat(x[0], w)))  # x[0] is features, x[1] is label\n",
        "\n",
        "        # Calculate cost and regularization\n",
        "        reg_term = lambda_reg * np.sum(w[:-1] ** 2)  # Regularization term\n",
        "\n",
        "        rdd_fcost = rdd_y_hat.map(lambda x: fcost(x[1], x[2]))  # Cost function value for each sample\n",
        "        J = rdd_fcost.reduce(lambda x, y: x + y)  # Sum up cost function across RDD\n",
        "        J += reg_term  # Add regularization term to total cost\n",
        "        J = J[0]\n",
        "        # Append current iteration's cost to list\n",
        "        costos_entrenamiento.append(J)\n",
        "\n",
        "        # Update weights\n",
        "        grad = rdd_y_hat.map(lambda x: grad_cost(x[1], x[0], x[2]))  # x[1] is label, x[0] is features, x[2] is y_hat\n",
        "        grad_sum = grad.reduce(lambda x, y: x + y)  # Sum up gradients across RDD\n",
        "\n",
        "        w = w - learning_rate * grad_sum - reg_term  # Update weight vector\n",
        "        print(f\"Iteration {i}  Cost:  {J}\")\n",
        "\n",
        "\n",
        "        end_time = time.time()  # End time of iteration\n",
        "        tiempo = end_time - start_time  # Duration of iteration\n",
        "        tiempos.append(tiempo)  # Append duration to list\n",
        "    return w, tiempos, costos_entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86b55f26",
      "metadata": {
        "id": "86b55f26"
      },
      "source": [
        "## Ejecucion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J6HRiSirUQw0",
      "metadata": {
        "id": "J6HRiSirUQw0"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de inicio\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3222c4e0",
      "metadata": {
        "id": "3222c4e0"
      },
      "outputs": [],
      "source": [
        "#Test\n",
        "\n",
        "ws,tiempos, costos_entrenamiento = train_visualizacion(data_normalized,nIter, learningRate,lambda_reg)\n",
        "acc = accuracy(data_normalized,ws)\n",
        "\n",
        "print(\"acc:\" , acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9b52700",
      "metadata": {
        "id": "a9b52700"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(costos_entrenamiento, label='Costo por Iteración')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Costo de Entrenamiento')\n",
        "plt.title('Costo de Entrenamiento por Iteración')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(tiempos, label='Tiempo por Iteración')\n",
        "plt.xlabel('Iteración')\n",
        "plt.ylabel('Tiempo (s)')\n",
        "plt.title('Tiempo de Ejecución por Iteración')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcf99f73",
      "metadata": {
        "id": "fcf99f73"
      },
      "outputs": [],
      "source": [
        "\n",
        "avg_acc = 0\n",
        "precisiones = []\n",
        "num_registros_train = []\n",
        "num_registros_test = []\n",
        "\n",
        "for i in range(num_block_cv):\n",
        "    print(f\"Cross-Validation con clave:{i}\")\n",
        "    train_data, test = get_block_data(data_cv, i)\n",
        "    num_registros_train.append(train_data.count())\n",
        "    num_registros_test.append(test.count())\n",
        "    ws = train(train_data, nIter, learningRate, lambda_reg)\n",
        "    acc = accuracy(test, ws)\n",
        "    precisiones.append(acc)\n",
        "    avg_acc += acc\n",
        "    print(\"Número de registros en train:\", num_registros_train[-1])\n",
        "    print(\"Número de registros en test:\", num_registros_test[-1])\n",
        "    print(\"acc:\", acc)\n",
        "\n",
        "avg_acc /= num_block_cv\n",
        "print(\"Average accuracy:\", avg_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "920aa10e",
      "metadata": {
        "id": "920aa10e"
      },
      "outputs": [],
      "source": [
        "num_registros_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca947038",
      "metadata": {
        "id": "ca947038"
      },
      "outputs": [],
      "source": [
        "num_registros_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40a71c8",
      "metadata": {
        "id": "e40a71c8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Bloque de CV')\n",
        "ax1.set_ylabel('Precisión', color=color)\n",
        "ax1.plot(precisiones, label='Precisión por bloque de CV', color=color, marker='o')\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "ax1.set_xticks(np.arange(num_block_cv))\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Número de registros', color=color)\n",
        "ax2.plot(num_registros_train, label='Registros en Train', color='blue', marker='x', linestyle='--')\n",
        "ax2.plot(num_registros_test, label='Registros en Test', color='cyan', marker='x', linestyle='--')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.tight_layout()\n",
        "fig.legend(loc=\"upper right\", bbox_to_anchor=(1,1), bbox_transform=ax1.transAxes)\n",
        "plt.title('Precisión y Número de Registros por Bloque de CV')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c0c4f61",
      "metadata": {
        "id": "1c0c4f61"
      },
      "outputs": [],
      "source": [
        "num_workers_list = [1, 2, 3, 4,5,6,7,8,9]\n",
        "tiempos_por_worker = []\n",
        "speedup_por_worker = []\n",
        "\n",
        "for num_workers in num_workers_list:\n",
        "    costos_entrenamiento = []\n",
        "    sc.stop()\n",
        "    sc = SparkContext(f\"local[{num_workers}]\", f\"Test_{num_workers}_workers\")\n",
        "\n",
        "    init_time = time.time()\n",
        "    #data = readFile(path)\n",
        "    #data = normalize(data)\n",
        "    #ws,tiempos, costos_entrenamiento = train_visualizacion(data_normalized,nIter, learningRate,lambda_reg)\n",
        "    #acc = accuracy(data_normalized, ws)\n",
        "    fin_time = time.time()\n",
        "    plt.xlabel(\"Iteración\")\n",
        "    plt.ylabel(\"Costo\")\n",
        "    plt.title(f\"Costo de entrenamiento con {num_workers}\")\n",
        "    plt.legend(num_workers_list, loc=\"best\")\n",
        "    plt.plot(costos_entrenamiento, color=f\"C{num_workers}\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    tiempo_total = fin_time - init_time\n",
        "    tiempos_por_worker.append(tiempo_total)\n",
        "\n",
        "    print(f\"Workers: {num_workers}, Acc: {acc}, Tiempo: {tiempo_total}\")\n",
        "\n",
        "    sc.stop()\n",
        "\n",
        "tiempo_con_1_worker = tiempos_por_worker[0]\n",
        "speedup_por_worker = [tiempo_con_1_worker / tiempo for tiempo in tiempos_por_worker]\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(num_workers_list, tiempos_por_worker, marker='o', linestyle='-', color='b')\n",
        "plt.xlabel('Número de Workers')\n",
        "plt.ylabel('Tiempo de ejecución (s)')\n",
        "plt.title('Curva de Rendimiento')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(num_workers_list, speedup_por_worker, marker='o', linestyle='-', color='r')\n",
        "plt.xlabel('Número de Workers')\n",
        "plt.ylabel('Speedup')\n",
        "plt.title('Curva de Speedup')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QBQ9Iye9UUnl",
      "metadata": {
        "id": "QBQ9Iye9UUnl"
      },
      "outputs": [],
      "source": [
        "# Medir el tiempo de finalización\n",
        "end_time = time.time()\n",
        "# Calcular y mostrar el tiempo de ejecución\n",
        "execution_time = end_time - start_time\n",
        "print(f'Tiempo de ejecución: {execution_time:.2f} segundos, {execution_time/60:.2f}  minutos')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e00EcwB1Opn0",
      "metadata": {
        "id": "e00EcwB1Opn0"
      },
      "source": [
        "# Analizar componentes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dU4pH0iwOxYs",
      "metadata": {
        "id": "dU4pH0iwOxYs"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "import subprocess\n",
        "\n",
        "# Obtener información del procesador\n",
        "cpu_info = os.popen(\"cat /proc/cpuinfo | grep 'model name' | uniq\").read().strip()\n",
        "print(f'Modelo de procesador: {cpu_info}')\n",
        "\n",
        "# Número de procesadores físicos\n",
        "num_processors = psutil.cpu_count(logical=False)\n",
        "print(f'Número de procesadores físicos: {num_processors}')\n",
        "\n",
        "# Número de vCores\n",
        "num_vcores = psutil.cpu_count(logical=True)\n",
        "print(f'Número de vCores (procesadores lógicos): {num_vcores}')\n",
        "\n",
        "# Capacidad de memoria\n",
        "mem = psutil.virtual_memory()\n",
        "total_memory_gb = mem.total / (1024 ** 3)  # Convertir bytes a GB\n",
        "available_memory_gb = mem.available / (1024 ** 3)  # Convertir bytes a GB\n",
        "print(f'Capacidad total de memoria RAM: {total_memory_gb:.2f} GB')\n",
        "print(f'Memoria RAM disponible: {available_memory_gb:.2f} GB')\n",
        "\n",
        "# Información del disco duro\n",
        "disk_usage = psutil.disk_usage('/')\n",
        "total_disk_gb = disk_usage.total / (1024 ** 3)  # Convertir bytes a GB\n",
        "used_disk_gb = disk_usage.used / (1024 ** 3)    # Convertir bytes a GB\n",
        "free_disk_gb = disk_usage.free / (1024 ** 3)    # Convertir bytes a GB\n",
        "print(f'Capacidad total del disco duro: {total_disk_gb:.2f} GB')\n",
        "print(f'Espacio utilizado del disco duro: {used_disk_gb:.2f} GB')\n",
        "print(f'Espacio libre del disco duro: {free_disk_gb:.2f} GB')\n",
        "\n",
        "# Tipo de disco duro\n",
        "disk_info = os.popen(\"lsblk -o NAME,ROTA,TYPE,SIZE | grep '^sda'\").read().strip()\n",
        "print(f'Tipo de disco duro: {disk_info}')\n",
        "\n",
        "# Información del nodo\n",
        "node_info = os.uname()\n",
        "print(f'Información del nodo: {node_info}')\n",
        "\n",
        "# Información detallada del sistema\n",
        "print(f'Información detallada del sistema:')\n",
        "print(f'Sistema: {node_info.sysname}')\n",
        "print(f'Nombre del nodo: {node_info.nodename}')\n",
        "print(f'Release: {node_info.release}')\n",
        "print(f'Versión: {node_info.version}')\n",
        "print(f'Máquina: {node_info.machine}')\n",
        "\n",
        "# Obtener información de la GPU\n",
        "try:\n",
        "    gpu_info = subprocess.check_output(\"nvidia-smi --query-gpu=name --format=csv,noheader\", shell=True).decode('utf-8').strip()\n",
        "    print(f'Modelo de GPU: {gpu_info}')\n",
        "except subprocess.CalledProcessError:\n",
        "    print('No se detectó GPU o NVIDIA-SMI no está instalado.')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}